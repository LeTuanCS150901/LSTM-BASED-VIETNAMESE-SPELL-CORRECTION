{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6xxaFKmkUcm"
      },
      "source": [
        "#0.TRUY CẬP FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYwo3-xQHSB5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixD73TzmHm4h",
        "outputId": "52efea43-dde2-4d1a-b218-be44ca862636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqyEn_41H3nH",
        "outputId": "01f04c98-f51e-41c9-c06b-454eafb8ae90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1RZiDa314dnAUksar0gWlPxznZpYfTFrV/HUCE_CS/CS_Nam_3/NLP\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/HUCE_CS/CS_Nam_3/NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w3z3cEFZhQ1",
        "outputId": "5b9064fc-65e3-4e93-94b8-7ef3fa9e6cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Using cached Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "Installing collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.4\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "# from unidecode import unidecode\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV7gnnYnZMns"
      },
      "source": [
        "#1.THU THẬP DỮ LIỆU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znTxFbf0YjM0"
      },
      "outputs": [],
      "source": [
        "class FileData(object):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        with open(path, encoding='utf-16') as f:\n",
        "          self.data = f.read()\n",
        "          #print(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62R5ekxBZoQR"
      },
      "outputs": [],
      "source": [
        "ABSOLUTE_PATH = \"/content/drive/MyDrive/HUCE_CS/CS_Nam_3/NLP/VNTC/Data/10Topics/Ver1.1/Test_Full\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lyYbU9AR_AO"
      },
      "outputs": [],
      "source": [
        "c_tri =  \"/Chinh tri Xa hoi\"\n",
        "\n",
        "d_song = \"/Doi song\"\n",
        "\n",
        "khoa_hoc = \"/Khoa hoc\"\n",
        "\n",
        "kinh_doanh = \"/Kinh doanh\"\n",
        "\n",
        "p_luat = \"/Phap luat\"\n",
        "\n",
        "suc_khoe = \"/Suc khoe\"\n",
        "\n",
        "the_gioi = \"/The gioi\"\n",
        "\n",
        "the_thao = \"/The thao\"\n",
        "\n",
        "van_hoa = \"/Van hoa\"\n",
        "\n",
        "vi_tinh = \"/Vi tinh\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ebuOhjqdR6A"
      },
      "outputs": [],
      "source": [
        "corpus = [c_tri, d_song, khoa_hoc, kinh_doanh, p_luat, suc_khoe, the_gioi, the_thao, van_hoa, vi_tinh]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdxyVSPJZ1eK"
      },
      "outputs": [],
      "source": [
        "for folder_path in range(len(corpus)):\n",
        "    corpus[folder_path] = ABSOLUTE_PATH + corpus[folder_path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w88ltME4SiP",
        "outputId": "72ea8bcd-0bcc-45c6-8f9f-b7a94be6438c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/HUCE_CS/CS_Nam_3/NLP/VNTC/Data/10Topics/Ver1.1/Test_Full/Khoa hoc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "corpus[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La-aEcptdPgo"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "file_list = []\n",
        "#count = 0\n",
        "\n",
        "for folder_path in corpus:\n",
        "    count = 0\n",
        "    for name in os.listdir(folder_path):\n",
        "        count +=1\n",
        "        if count == 1500:\n",
        "          break        \n",
        "        path = os.path.join(folder_path, name)\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "        file = FileData(path)\n",
        "        file_list.append( file.data )\n",
        "        # count +=1\n",
        "        # if count == 5:\n",
        "        #   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bpdg-jn87df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8fc0bd-dbd9-4651-b6a8-e769f8a7450d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14990"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzFpHKHaP0l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb890f8-3f94-4fe2-ec97-9e12b790af16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-983dcdc4c485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpk_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pk_path' is not defined"
          ]
        }
      ],
      "source": [
        "with open(pk_path, \"wb\") as f:\n",
        "    cPickle.dump(file_list, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMAmC-ktKreu"
      },
      "source": [
        "# 2.IMPORT DU LIEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUU719ppW0_b"
      },
      "outputs": [],
      "source": [
        "import pickle as cPickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRph_nIndXi9"
      },
      "outputs": [],
      "source": [
        "pk_path = '/content/drive/MyDrive/HUCE_CS/CS_Nam_3/NLP/VNTC/Data/10Topics/Ver1.1/train_data.p'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dr2HuIqQBIB"
      },
      "outputs": [],
      "source": [
        "with open(pk_path, \"rb\") as f:\n",
        "    data = cPickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jXs1dPCRcPd"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YidyP3TqVxKu"
      },
      "outputs": [],
      "source": [
        "data[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXUdGdxKxw3"
      },
      "source": [
        "##2.1 TIEN XU LY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPo_wynOJgtM"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9f62EMn7CLV"
      },
      "outputs": [],
      "source": [
        "alphabet = '^[ _abcdefghijklmnopqrstuvwxyz0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ!\\\"\\',\\-\\.:;?_\\(\\)]+$'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unakwWmbLmrN"
      },
      "source": [
        "Xây dựng bộ ngữ liệu là 1 list chứa các câu. Mỗi từ trong các câu đều phải là chữ cái Latin. Nếu 1 câu bất kì chứa 1 từ không thuộc chữ cái Latin thì ta không chọn câu đấy vào tập ngữ liệu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOhfO5RzV1GW"
      },
      "outputs": [],
      "source": [
        "def latin_extract(data):\n",
        "\n",
        "    # extract Latin- characters only\n",
        "   \n",
        "    latin_extract_data=[]\n",
        "    # duyet qua tung van ban\n",
        "    for i in data:\n",
        "      if i == 1:\n",
        "        break\n",
        "      # thay the xuong dong la dau cham ket thuc\n",
        "      i=i.replace(\"\\n\",\".\")\n",
        "      # tach van ban theo dau cham ket thuc\n",
        "      sentences=i.split(\".\")\n",
        "      for j in sentences:\n",
        "          #print(\"j hien tai = \", j)\n",
        "          #print( \"split=\", j.split() )\n",
        "          if len(j.split()) > 2 and re.match(alphabet, j.lower()):\n",
        "            \n",
        "              latin_extract_data.append(j)\n",
        "              #print(\"j moi = \",j)\n",
        "\n",
        "    return latin_extract_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7_YqN_DJYUA"
      },
      "outputs": [],
      "source": [
        "training_data = latin_extract(data)\n",
        "\n",
        "print(len(training_data))\n",
        "training_data[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp6NYXjjPPm8"
      },
      "outputs": [],
      "source": [
        "i = 100\n",
        "while i < 110:\n",
        "  print(training_data[i])\n",
        "  i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9x_QXFoJrlC"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode # this module removes tones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw2dp5NzJpp5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6t2kr8pai6l"
      },
      "outputs": [],
      "source": [
        "# some common Vietnamese spell mistake\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "letters2=list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "# danh may\n",
        "typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n",
        "\"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n",
        "\"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n",
        "\"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n",
        "\"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n",
        "\"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n",
        "\"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n",
        "\"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n",
        "\"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n",
        "\"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n",
        "\n",
        "# dia phuong\n",
        "region={\"ẻ\":\"ẽ\",\"ẽ\":\"ẻ\",\"ũ\":\"ủ\",\"ủ\":\"ũ\",\"ã\":\"ả\",\"ả\":\"ã\",\"ỏ\":\"õ\",\"õ\":\"ỏ\",\"i\":\"j\"}\n",
        "region2={\"s\":\"x\",\"l\":\"n\",\"n\":\"l\",\"x\":\"s\",\"d\":\"gi\",\"S\":\"X\",\"L\":\"N\",\"N\":\"L\",\"X\":\"S\",\"Gi\":\"D\",\"D\":\"Gi\"}\n",
        "\n",
        "# nguyen am\n",
        "vowel=list(\"aeiouyáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵ\")\n",
        "\n",
        "# viet tat\n",
        "acronym={\"không\":\"ko\",\" anh\":\" a\",\"em\":\"e\",\"biết\":\"bít\",\"giờ\":\"h\",\"gì\":\"j\",\"muốn\":\"mún\",\"học\":\"hok\",\"yêu\":\"iu\",\n",
        "         \"chồng\":\"ck\",\"vợ\":\"vk\",\" ông\":\" ô\",\"được\":\"đc\",\"tôi\":\"t\",\n",
        "         \"Không\":\"Ko\",\" Anh\":\" A\",\"Em\":\"E\",\"Biết\":\"Bít\",\"Giờ\":\"H\",\"Gì\":\"J\",\"Muốn\":\"Mún\",\"Học\":\"Hok\",\"Yêu\":\"Iu\",\n",
        "         \"Chồng\":\"Ck\",\"Vợ\":\"Vk\",\" Ông\":\" Ô\",\"Được\":\"Đc\",\"Tôi\":\"T\",}\n",
        "\n",
        "# teencode\n",
        "teen={\"ch\":\"ck\",\"ph\":\"f\",\"th\":\"tk\",\"nh\":\"nk\",\n",
        "      \"Ch\":\"Ck\",\"Ph\":\"F\",\"Th\":\"Tk\",\"Nh\":\"Nk\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pob_Cy9oarAs"
      },
      "outputs": [],
      "source": [
        "# function for adding mistake( noise)\n",
        "def teen_code(sentence,pivot):\n",
        "    random = np.random.uniform(0,1,1)[0]\n",
        "    new_sentence=str(sentence)\n",
        "    if random>pivot:\n",
        "        for word in acronym.keys():\n",
        "            if re.search(word, new_sentence):\n",
        "                random2 = np.random.uniform(0,1,1)[0]\n",
        "                if random2 <0.5:\n",
        "                    new_sentence=new_sentence.replace(word,acronym[word])\n",
        "        for word in teen.keys(): \n",
        "            if re.search(word, new_sentence):\n",
        "                random3 = np.random.uniform(0,1,1)[0]\n",
        "                if random3 <0.05:\n",
        "                    new_sentence=new_sentence.replace(word,teen[word])        \n",
        "        return new_sentence\n",
        "    else:\n",
        "        return sentence\n",
        "    \n",
        "\n",
        "def add_noise(sentence, pivot1,pivot2):\n",
        "    sentence=teen_code(sentence,0.5)\n",
        "    noisy_sentence = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        if sentence[i] not in letters:\n",
        "            noisy_sentence+=sentence[i]\n",
        "        else: \n",
        "            random = np.random.uniform(0,1,1)[0]   \n",
        "            if random < pivot1:\n",
        "                noisy_sentence+=(sentence[i])\n",
        "            elif random<pivot2:\n",
        "                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n",
        "                    random2=np.random.uniform(0,1,1)[0]\n",
        "                    if random2<=0.4:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random2<0.8:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random2<0.95 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in typo.keys():\n",
        "                    random3=np.random.uniform(0,1,1)[0]\n",
        "                    if random3<=0.6:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random3<0.9 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])                        \n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in region.keys():\n",
        "                    random4=np.random.uniform(0,1,1)[0]\n",
        "                    if random4<=0.6:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random4<0.85 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])                        \n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif i<len(sentence)-1 :\n",
        "                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n",
        "                        random5=np.random.uniform(0,1,1)[0]\n",
        "                        if random5<=0.9:\n",
        "                            noisy_sentence+=region2[sentence[i]]\n",
        "                        else:\n",
        "                            noisy_sentence+=sentence[i]\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "\n",
        "            else:\n",
        "                new_random = np.random.uniform(0,1,1)[0]\n",
        "                if new_random <=0.33:\n",
        "                    if i == (len(sentence) - 1):\n",
        "                        continue\n",
        "                    else:\n",
        "                        noisy_sentence+=(sentence[i+1])\n",
        "                        noisy_sentence+=(sentence[i])\n",
        "                        i += 1\n",
        "                elif new_random <= 0.66:\n",
        "                    random_letter = np.random.choice(letters2, 1)[0]\n",
        "                    noisy_sentence+=random_letter\n",
        "                else:\n",
        "                    pass\n",
        "      \n",
        "        i += 1\n",
        "    return noisy_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QToUfBLCdRd8"
      },
      "source": [
        "Tách các cụm từ (phrases), sử dụng biểu thức chính quy. Nếu trong 1 câu được ngăn cách bởi ký tự đặc biệt thì ta sẽ xóa từ chứa ký tự đặc biệt và tách các cụm từ đã bị ngăn cách trước đó \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb4qlCgYym0r"
      },
      "outputs": [],
      "source": [
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g61rIjlnauFP"
      },
      "outputs": [],
      "source": [
        "def extract_phrases(text):\n",
        "    return re.findall(r'\\w[\\w ]+', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgvHdn9Na_7I"
      },
      "outputs": [],
      "source": [
        "def _extract_phrases(data):\n",
        "    phrases = itertools.chain.from_iterable(extract_phrases(text) for text in data)\n",
        "    phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n",
        "\n",
        "    return phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXFni6LLy5W_"
      },
      "outputs": [],
      "source": [
        "phrases = _extract_phrases(training_data)\n",
        "\n",
        "print(len(phrases))\n",
        "print(phrases[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXnuL9DJfPDs"
      },
      "source": [
        "Chia tập ngữ điệu thành 2-gram. Từng phần tử trong list gồm có 2 từ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m2uUHJQ6ZdT"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "import string\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T11TNxY_6xde"
      },
      "outputs": [],
      "source": [
        "# divide document into 2-grams \n",
        "# a single Vietnamese word cant contain more than 7 characters ( nghiêng )\n",
        "NGRAM = 2 \n",
        "MAXLEN = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyWAljsu2t7b"
      },
      "outputs": [],
      "source": [
        "def gen_ngrams(words, n=2):\n",
        "    return ngrams(words.split(), n)\n",
        "\n",
        "def generate_bi_grams(phrases):   \n",
        "    list_ngrams = []\n",
        "    for p in tqdm(phrases):\n",
        "  \n",
        "      # neu khong nham trong bang chu cai thi bo qua\n",
        "      if not re.match(alphabet, p.lower()):\n",
        "        continue\n",
        "\n",
        "      # tach p thanh cac bi gram   \n",
        "      for ngr in gen_ngrams(p, NGRAM):\n",
        "        if len(\" \".join(ngr)) < MAXLEN:\n",
        "          list_ngrams.append(\" \".join(ngr))\n",
        "\n",
        "    return list_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwTIxtQs21Im"
      },
      "outputs": [],
      "source": [
        "list_ngrams = generate_bi_grams(phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7O_Ht4F7yum"
      },
      "outputs": [],
      "source": [
        "print(len(list_ngrams))\n",
        "print(list_ngrams[7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtBfrCAjh8s8"
      },
      "source": [
        "Mã hóa câu thành ma trận 2 chiều và giải mã câu đó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX8jC5BRe8uy"
      },
      "outputs": [],
      "source": [
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "print(alphabet)\n",
        "print(len(alphabet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNZKniR1fhZj"
      },
      "outputs": [],
      "source": [
        "# So a 2-grams contain at most 7*2 = 14 character (except one that has spell mistake)\n",
        "# add \"\\x00\" padding at the end of 2-grams in order to equal their length\n",
        "def encoder_data(text, maxlen=MAXLEN):\n",
        "        #print(\"Maxlen\", maxlen)\n",
        "        text = \"\\x00\" + text\n",
        "        #print(\"text\", text)\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        #print(\"X ban dau\", x)\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "          for j in range(i+1, maxlen):\n",
        "            x[j, 0] = 1\n",
        "        return x\n",
        "      \n",
        "def decoder_data(x):\n",
        "    x = x.argmax(axis=-1)\n",
        "    #print(\"x hien tai\", x)\n",
        "    dem = ''.join(alphabet[i] for i in x)\n",
        "    #print(\"Do dai cau van\", len(dem))\n",
        "\n",
        "    return dem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC_A8rc6gKwC"
      },
      "outputs": [],
      "source": [
        "print(encoder_data(\"Tôi tên là LÊ TUẤN\").shape)\n",
        "print(decoder_data(encoder_data(\"Tôi tên là LÊ TUẤN\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXt3D_tbisN2"
      },
      "source": [
        "#3.BUILD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43MbypBriuwJ"
      },
      "outputs": [],
      "source": [
        "# Build the neural network\n",
        "# this is adapted from the seq2seq architecture, which can be used for Machine Translation, Text Summarization Image Captioning ...\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense,LSTM, Bidirectional\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam # - Works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-IeuGbS5vOl"
      },
      "outputs": [],
      "source": [
        "MAXLEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URGzW2go5wTn"
      },
      "outputs": [],
      "source": [
        "len(alphabet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPyg_Q2siyJr"
      },
      "outputs": [],
      "source": [
        "encoder = LSTM(256,input_shape=(MAXLEN, len(alphabet)), return_sequences=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zhW5WUyjEPn"
      },
      "outputs": [],
      "source": [
        "decoder=Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIrnqBxKjGU6"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(encoder)\n",
        "model.add(decoder)\n",
        "model.add(TimeDistributed(Dense(256)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(TimeDistributed(Dense(len(alphabet))))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uL7KFl_jIgb"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoK3qdpujKqQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Wj2G2gjOak"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFoVo_yDjvIQ"
      },
      "outputs": [],
      "source": [
        "len(list_ngrams)\n",
        "list_ngrams[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w94uZcE_jvwK"
      },
      "outputs": [],
      "source": [
        "print(len(list_ngrams))\n",
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzirJBY5kqpX"
      },
      "outputs": [],
      "source": [
        "# we have to use data- generation medthod cause this dataset is too large to fit into memory\n",
        "BATCH_SIZE = 512\n",
        "def generate_data(data, batch_size):\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        x, y = [], []\n",
        "        for i in range(batch_size):  \n",
        "            y.append(encoder_data(data[cur_index]))\n",
        "            x.append(encoder_data(add_noise(data[cur_index],0.94,0.985)))\n",
        "            cur_index += 1\n",
        "            if cur_index > len(data)-1:\n",
        "                cur_index = 0\n",
        "        yield np.array(x), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_t-VOdWk4PI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-9-dLlLk6Ca"
      },
      "outputs": [],
      "source": [
        "# train the model and save to the Model folder\n",
        "checkpointer = ModelCheckpoint(filepath=os.path.join('./model/spell_{val_acc:.2f}.h5'), save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkqPYVcGlc1z"
      },
      "outputs": [],
      "source": [
        "model.fit_generator( train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=10,\n",
        "                    validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE,\n",
        "                    callbacks=[checkpointer] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4oXdRcLuEgw"
      },
      "source": [
        "#4.ĐÁNH GIÁ MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8N2JLDJ8k8d"
      },
      "outputs": [],
      "source": [
        "test_data_path = '/content/drive/MyDrive/HUCE_CS/CS_Nam_3/NLP/VNTC/Data/10Topics/Ver1.1/test_data.p'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6NDrdSg9NLd"
      },
      "outputs": [],
      "source": [
        "with open(pk_path, \"rb\") as f:\n",
        "    test_data = cPickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r2bF4dM9w_W"
      },
      "outputs": [],
      "source": [
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV0W_l2Kv5v-"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daWgIg4_v-BI"
      },
      "outputs": [],
      "source": [
        "model_link = \"/content/drive/MyDrive/HUCE_CS/CS_Nam_3/NLP/Vietnamese-Spell-Correction/model/spell_0.99.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyI6hUtAuIw7"
      },
      "outputs": [],
      "source": [
        "eval_model = load_model(model_link)\n",
        "eval_model.evaluate_generator(test_generator,steps=len(test)//BATCH_SIZE,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVBZe0a79FyR"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRewGYVd-GIJ"
      },
      "source": [
        "#5.DỰ ĐOÁN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "id": "4F4V_yigjND-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOYH9uWF-OAf"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from keras.models import load_model\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams,word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import unidecode\n",
        "import string\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zajwNyjRAVtK"
      },
      "outputs": [],
      "source": [
        "link_model = \"/content/drive/MyDrive/HUCE_CS/CS_Nam_3/NLP/Vietnamese-Spell-Correction/model/spell_0.99.h5\"\n",
        "model = load_model(link_model)\n",
        "model.make_predict_function()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XjsEp02AduP"
      },
      "outputs": [],
      "source": [
        "NGRAM=2\n",
        "MAXLEN=40 # INPUT SHAPE\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "accepted_char=list((string.digits + ''.join(letters)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call(sentence):\n",
        "    def extract_phrases(text):\n",
        "        pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n",
        "        return re.findall(pattern, text)\n",
        "\n",
        "    def encoder_data(text, maxlen=MAXLEN):\n",
        "            text = \"\\x00\" + text\n",
        "            x = np.zeros((maxlen, len(alphabet)))\n",
        "            for i, c in enumerate(text[:maxlen]):\n",
        "                x[i, alphabet.index(c)] = 1\n",
        "            if i < maxlen - 1:\n",
        "              for j in range(i+1, maxlen):\n",
        "                x[j, 0] = 1\n",
        "            return x    \n",
        "            \n",
        "    def decoder_data(x):\n",
        "        x = x.argmax(axis=-1)\n",
        "        return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "    def nltk_ngrams(words, n=2):\n",
        "        return ngrams(words.split(), n)\n",
        "\n",
        "    def guess(ngram):\n",
        "        text = ' '.join(ngram)\n",
        "        preds = model.predict(np.array([encoder_data(text)]), verbose=0)\n",
        "        return decoder_data(preds[0]).strip('\\x00')\n",
        "\n",
        "    def correct(sentence):\n",
        "        for i in sentence:\n",
        "            if i not in accepted_char:\n",
        "                sentence=sentence.replace(i,\" \")\n",
        "        ngrams = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "        guessed_ngrams = list(guess(ngram) for ngram in ngrams)\n",
        "\n",
        "        print(\"N gram\", ngrams)\n",
        "        print(\"guess\", guessed_ngrams)\n",
        "\n",
        "        #return guessed_ngrams\n",
        "\n",
        "        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "        for nid, ngram in (enumerate(guessed_ngrams)):\n",
        "            for wid, word in (enumerate(re.split(' +', ngram))):\n",
        "                candidates[nid + wid].update([word])\n",
        "\n",
        "        # for c in candidates:\n",
        "        #     print(c.most_common(1))        \n",
        "        output = ' '.join(c.most_common(1)[0][0] for c in candidates)  \n",
        "        return output\n",
        "\n",
        "    guess = correct(sentence)\n",
        "\n",
        "    return guess"
      ],
      "metadata": {
        "id": "JCLDKMzPl19O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEibQsCG-aLS"
      },
      "outputs": [],
      "source": [
        "# sentence = \"Các phát thank viên dẫn trương trink\"\n",
        "sentence = \"Hom lay tooi đi hoc\"\n",
        "guess = call(sentence)\n",
        "# guess = correct(sentence)\n",
        "print(guess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzgN-xZXWdgV"
      },
      "source": [
        "#6.BUILD WEB APP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCZhZwz1WoPB"
      },
      "outputs": [],
      "source": [
        "!pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "i31DJzyWiMuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def spell_predict(text):\n",
        "    guess = call(text)\n",
        "    return guess\n",
        "\n",
        "demo = gr.Interface(\n",
        "                    fn=spell_predict, \n",
        "                    inputs=\"text\",\n",
        "                    outputs=\"text\",\n",
        "                    title=\"BẢO VỆ ĐỒ ÁN XỬ LÝ NGÔN NGỮ TỰ NHIÊN - NHÓM 6\",\n",
        "                    description=\"Nhập câu bất kì để tự động được sửa lỗi chính tả\"\n",
        "                    )\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "RYdfxGvaiisw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3AHXHs-iwzk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gV7gnnYnZMns",
        "GMAmC-ktKreu",
        "hEXUdGdxKxw3",
        "CXt3D_tbisN2",
        "g4oXdRcLuEgw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}